{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QPO9EWW2zrc",
        "outputId": "062a502b-6e39-446b-c01b-9e9abb5bf4cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers streamlit -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import datetime\n",
        "\n",
        "st.set_page_config(page_title=\"Motivational Quote Generator\", page_icon=\"💡\")\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    model_id = \"tiiuae/falcon-rw-1b\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "generator = load_model()\n",
        "\n",
        "moods = [\n",
        "    \"Anxious\", \"Stressed\", \"Unmotivated\", \"Tired\",\n",
        "    \"Excited\", \"Confused\", \"Focused\", \"Sad\", \"Lonely\"\n",
        "]\n",
        "\n",
        "def generate_quote(mood):\n",
        "    prompt = f\"Give me a short motivational quote for someone who is feeling {mood.lower()}.\"\n",
        "    output = generator(prompt, max_new_tokens=60, do_sample=True, temperature=0.9)[0][\"generated_text\"]\n",
        "    return output.replace(prompt, \"\").strip()\n",
        "\n",
        "st.title(\"💬 Motivational Quote Generator\")\n",
        "st.write(\"Select your current **mood** to receive a personalized motivational quote.\")\n",
        "\n",
        "selected_mood = st.selectbox(\"🌈 How are you feeling?\", moods)\n",
        "\n",
        "if st.button(\"✨ Generate Quote\"):\n",
        "    with st.spinner(\"Generating your quote...\"):\n",
        "        quote = generate_quote(selected_mood)\n",
        "        st.success(\"Here is your quote!\")\n",
        "        st.markdown(f\"**🧠 Mood:** {selected_mood}\")\n",
        "        st.markdown(f\"**🗣️ Quote:** _{quote}_\")\n",
        "\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        with open(\"quotes.txt\", \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"[{timestamp}] Mood: {selected_mood} → {quote}\\n\")\n",
        "\n",
        "        st.info(\"✅ This quote has been saved to `quotes.txt`.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hggFq3A3EW8",
        "outputId": "198a3b54-1a1b-4992-c488-dffea531c84f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2zMzrE9t1WjAtkvfskEzMlzbXOZ_UoUCrsuYqB15d1pLyZ5j-"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn8GZTdY4ilF",
        "outputId": "7be6f6ef-0002-479a-f1ba-223420d410c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()\n",
        "public_url = ngrok.connect(addr=\"8501\", proto=\"http\")\n",
        "print(f\"🌐 Public URL: {public_url}\")\n",
        "!streamlit run app.py &> /dev/null &\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klj03taL4o1B",
        "outputId": "2a4c88f4-df97-4c94-93cd-5db3ac6f862e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌐 Public URL: NgrokTunnel: \"https://09fd-34-41-178-31.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}